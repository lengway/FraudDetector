{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b448cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb40bfc",
   "metadata": {},
   "source": [
    "# ðŸ“Š ÐŸÐ¾Ð»Ð½Ñ‹Ð¹ Ð°Ð½Ð°Ð»Ð¸Ð· Two-Stage Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779942c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ð’Ð¡Ð• Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹\n",
    "all_results = pd.read_csv(\"../docs/two_stage_detection_results.csv\")\n",
    "ml_results = pd.read_csv(\"../docs/ml_checked_transactions.csv\")\n",
    "\n",
    "print(f\"ðŸ“Š Total transactions: {len(all_results)}\")\n",
    "print(f\"ðŸ“Š ML-checked: {len(ml_results)}\")\n",
    "print(f\"ðŸ“Š Auto-approved: {len(all_results) - len(ml_results)}\")\n",
    "\n",
    "# Ð’ÐÐ–ÐÐž! ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ ÑÐºÐ¾Ð»ÑŒÐºÐ¾ Ð’Ð¡Ð•Ð“Ðž Ñ„Ñ€Ð¾Ð´ÐµÑ€Ð¾Ð²\n",
    "if 'target' in all_results.columns:\n",
    "    total_frauds = all_results['target'].sum()\n",
    "    ml_checked_frauds = ml_results['target'].sum() if 'target' in ml_results.columns else 0\n",
    "    auto_approved_frauds = total_frauds - ml_checked_frauds\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ FRAUD DISTRIBUTION:\")\n",
    "    print(f\"   Total frauds in dataset: {total_frauds}\")\n",
    "    print(f\"   Sent to ML check: {ml_checked_frauds}\")\n",
    "    print(f\"   âŒ Auto-approved (MISSED by scorecard): {auto_approved_frauds}\")\n",
    "    print(f\"\\n   ðŸ“Š Scorecard Recall: {ml_checked_frauds/total_frauds*100:.1f}%\")\n",
    "    print(f\"      (Ð˜Ð· {total_frauds} Ñ„Ñ€Ð¾Ð´Ð¾Ð² Ð¾Ñ‚Ð¿Ñ€Ð°Ð²Ð¸Ð» {ml_checked_frauds} Ð½Ð° ML-Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÑƒ)\")\n",
    "\n",
    "all_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d78b17a",
   "metadata": {},
   "source": [
    "## ÐÐ½Ð°Ð»Ð¸Ð· ML-Ð¼Ð¾Ð´ÐµÐ»Ð¸ (Ð½Ð° Ñ‚ÐµÑ… ÐºÑ‚Ð¾ Ð¿Ñ€Ð¾ÑˆÐµÐ» scorecard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d95b570",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'target' in ml_results.columns and 'fraud_prediction' in ml_results.columns:\n",
    "    from sklearn.metrics import confusion_matrix, classification_report\n",
    "    \n",
    "    actual = ml_results['target'].sum()\n",
    "    predicted = ml_results['fraud_prediction'].sum()\n",
    "    \n",
    "    print(f\"\\nðŸ¤– ML MODEL RESULTS (on {len(ml_results)} checked transactions):\")\n",
    "    print(f\"   Actual frauds: {actual}\")\n",
    "    print(f\"   Predicted frauds: {predicted}\")\n",
    "    print(f\"   Difference: {predicted - actual}\")\n",
    "    \n",
    "    cm = confusion_matrix(ml_results['target'], ml_results['fraud_prediction'])\n",
    "    print(f\"\\n   Confusion Matrix:\")\n",
    "    print(f\"   TN: {cm[0,0]} | FP: {cm[0,1]}\")\n",
    "    print(f\"   FN: {cm[1,0]} | TP: {cm[1,1]}\")\n",
    "    \n",
    "    print(f\"\\n   ðŸ’¡ ÐžÐ±ÑŠÑÑÐ½ÐµÐ½Ð¸Ðµ:\")\n",
    "    print(f\"   Predicted ({predicted}) = TP ({cm[1,1]}) + FP ({cm[0,1]})\")\n",
    "    print(f\"   TP = Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ð¾ Ð½Ð°ÑˆÐ»Ð¸ Ñ„Ñ€Ð¾Ð´\")\n",
    "    print(f\"   FP = Ð¾ÑˆÐ¸Ð±Ð¾Ñ‡Ð½Ð¾ Ð½Ð°Ð·Ð²Ð°Ð»Ð¸ Ñ„Ñ€Ð¾Ð´Ð¾Ð¼ (false alarm)\")\n",
    "    print(f\"   FN = Ð¿Ñ€Ð¾Ð¿ÑƒÑÑ‚Ð¸Ð»Ð¸ Ñ„Ñ€Ð¾Ð´\")\n",
    "    \n",
    "    print(\"\\n   Classification Report:\")\n",
    "    print(classification_report(ml_results['target'], ml_results['fraud_prediction'], \n",
    "                                target_names=['Non-Fraud', 'Fraud']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec057c1",
   "metadata": {},
   "source": [
    "## Ð˜Ð¢ÐžÐ“ÐžÐ’ÐÐ¯ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° Ð¿Ð¾ Ð’Ð¡Ð•Ðœ Ñ„Ñ€Ð¾Ð´Ð°Ð¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3be35a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'target' in all_results.columns:\n",
    "    total_frauds = all_results['target'].sum()\n",
    "    \n",
    "    # Ð¡ÐºÐ¾Ð»ÑŒÐºÐ¾ Ð¿Ð¾Ð¹Ð¼Ð°Ð»Ð¸\n",
    "    if 'fraud_prediction' in all_results.columns:\n",
    "        detected = all_results['fraud_prediction'].sum()\n",
    "        \n",
    "        # Ð ÐµÐ°Ð»ÑŒÐ½Ð¾ Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ð¾ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð½Ñ‹Ðµ\n",
    "        true_detected = ((all_results['target'] == 1) & (all_results['fraud_prediction'] == 1)).sum()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ðŸŽ¯ FINAL SUMMARY (entire pipeline)\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Total frauds in dataset: {total_frauds}\")\n",
    "        print(f\"Frauds detected: {true_detected}\")\n",
    "        print(f\"Frauds missed: {total_frauds - true_detected}\")\n",
    "        print(f\"\\nOverall Recall: {true_detected/total_frauds*100:.1f}%\")\n",
    "        print(f\"Overall Precision: {true_detected/detected*100:.1f}%\" if detected > 0 else \"N/A\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Ð”ÐµÑ‚Ð°Ð»Ð¸ Ð¿Ñ€Ð¾Ð¿ÑƒÑ‰ÐµÐ½Ð½Ñ‹Ñ…\n",
    "        missed = all_results[(all_results['target'] == 1) & (all_results['fraud_prediction'] == 0)]\n",
    "        if len(missed) > 0:\n",
    "            print(f\"\\nâŒ MISSED FRAUDS ({len(missed)}):\")\n",
    "            if 'detection_stage' in missed.columns:\n",
    "                print(missed['detection_stage'].value_counts())\n",
    "            if 'scorecard_total' in missed.columns:\n",
    "                print(f\"\\nTheir scorecard scores:\")\n",
    "                print(missed['scorecard_total'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59450e25",
   "metadata": {},
   "source": [
    "## False Positives Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c52af58",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'target' in all_results.columns and 'fraud_prediction' in all_results.columns:\n",
    "    fp = all_results[(all_results['target'] == 0) & (all_results['fraud_prediction'] == 1)]\n",
    "    \n",
    "    print(f\"\\nðŸ” FALSE POSITIVES: {len(fp)}\")\n",
    "    \n",
    "    if len(fp) > 0 and 'fraud_probability' in fp.columns:\n",
    "        print(f\"   Mean probability: {fp['fraud_probability'].mean():.3f}\")\n",
    "        \n",
    "        if 'scorecard_total' in fp.columns:\n",
    "            print(f\"\\n   Scorecard distribution:\")\n",
    "            print(fp['scorecard_total'].value_counts().sort_index())\n",
    "        \n",
    "        print(f\"\\n   Top 5 examples:\")\n",
    "        display_cols = ['fraud_probability', 'scorecard_total', 'rare_os_flag', 'rare_device_flag']\n",
    "        display_cols = [c for c in display_cols if c in fp.columns]\n",
    "        print(fp[display_cols].head())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
