"""
Data Loading and Preprocessing Module
Centralized data loading to avoid code duplication
"""

import pandas as pd
import numpy as np
import os
import warnings
warnings.filterwarnings('ignore')


def clean_columns(df):
    """Remove spaces and BOM markers from column names."""
    df.columns = (
        df.columns.astype(str)
        .str.strip()
        .str.replace('\ufeff', '')
        .str.replace('"', '')
    )
    return df


def load_data(base_path='docs'):
    """
    Load transaction and behavioral data from CSV files.
    
    Args:
        base_path: Directory containing CSV files (default: 'docs')
    
    Returns:
        tuple: (df_transactions, df_behavioral)
    
    Raises:
        FileNotFoundError: If CSV files are not found
    """
    print("üìä Loading data...")
    
    # Try different paths
    trans_paths = [
        os.path.join(base_path, '—Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ –≤ –ú–æ–±–∏–ª—å–Ω–æ–º –∏–Ω—Ç–µ—Ä–Ω–µ—Ç –ë–∞–Ω–∫–∏–Ω–≥–µ.csv'),
        '—Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ –≤ –ú–æ–±–∏–ª—å–Ω–æ–º –∏–Ω—Ç–µ—Ä–Ω–µ—Ç –ë–∞–Ω–∫–∏–Ω–≥–µ.csv'
    ]
    
    behav_paths = [
        os.path.join(base_path, '–ø–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∫–ª–∏–µ–Ω—Ç–æ–≤.csv'),
        '–ø–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∫–ª–∏–µ–Ω—Ç–æ–≤.csv'
    ]
    
    # Load transactions
    df_trans = None
    for path in trans_paths:
        if os.path.exists(path):
            try:
                df_trans = pd.read_csv(
                    path,
                    sep=';',
                    encoding='cp1251',
                    header=1,
                    engine='python',
                )
                df_trans = clean_columns(df_trans)
                print(f"‚úì Loaded transactions from {path} (shape: {df_trans.shape})")
                break
            except Exception as e:
                print(f"‚ö†Ô∏è Failed to load {path}: {e}")
                continue
    
    if df_trans is None:
        raise FileNotFoundError(f"Transaction CSV not found in paths: {trans_paths}")
    
    # Load behavioral patterns
    df_behavior = None
    for path in behav_paths:
        if os.path.exists(path):
            try:
                df_behavior = pd.read_csv(
                    path,
                    sep=';',
                    encoding='cp1251',
                    header=0,
                    engine='python',
                    on_bad_lines='skip',
                )
                df_behavior = clean_columns(df_behavior)
                print(f"‚úì Loaded behavioral data from {path} (shape: {df_behavior.shape})")
                break
            except Exception as e:
                print(f"‚ö†Ô∏è Failed to load {path}: {e}")
                continue
    
    if df_behavior is None:
        raise FileNotFoundError(f"Behavioral CSV not found in paths: {behav_paths}")
    
    return df_trans, df_behavior


def clean_and_merge(df_trans, df_behavior):
    """
    Clean column names, normalize data types, and merge datasets.
    
    Args:
        df_trans: Transaction DataFrame
        df_behavior: Behavioral patterns DataFrame
    
    Returns:
        pd.DataFrame: Merged and cleaned dataset
    """
    print("üîß Cleaning and merging data...")
    
    # Rename behavioral columns for consistency
    behav_map = {
        '–£–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∫–ª–∏–µ–Ω—Ç–∞': 'cst_dim_id',
        '–î–∞—Ç–∞ —Å–æ–≤–µ—Ä—à–µ–Ω–Ω–æ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏': 'transdate',
        '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–∞–∑–Ω—ã—Ö –≤–µ—Ä—Å–∏–π –û–° (os_ver) –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –¥–Ω–µ–π –¥–æ transdate ‚Äî —Å–∫–æ–ª—å–∫–æ —Ä–∞–∑–Ω—ã—Ö –û–°/–≤–µ—Ä—Å–∏–π –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª –∫–ª–∏–µ–Ω—Ç': 'os_count_30d',
        '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Ç–µ–ª–µ—Ñ–æ–Ω–∞ (phone_model) –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –¥–Ω–µ–π ‚Äî –Ω–∞—Å–∫–æ–ª—å–∫–æ —á–∞—Å—Ç–æ –∫–ª–∏–µ–Ω—Ç "–º–µ–Ω—è–ª —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ" –ø–æ –ª–æ–≥–∞–º': 'device_count_30d',
        '–ú–æ–¥–µ–ª—å —Ç–µ–ª–µ—Ñ–æ–Ω–∞ –∏–∑ —Å–∞–º–æ–π –ø–æ—Å–ª–µ–¥–Ω–µ–π —Å–µ—Å—Å–∏–∏ (–ø–æ –≤—Ä–µ–º–µ–Ω–∏) –ø–µ—Ä–µ–¥ transdate': 'last_phone_model',
        '–í–µ—Ä—Å–∏—è –û–° –∏–∑ —Å–∞–º–æ–π –ø–æ—Å–ª–µ–¥–Ω–µ–π —Å–µ—Å—Å–∏–∏ –ø–µ—Ä–µ–¥ transdate': 'last_os_ver',
        '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ª–æ–≥–∏–Ω-—Å–µ—Å—Å–∏–π (–º–∏–Ω—É—Ç–Ω—ã—Ö —Ç–∞–π–º-—Å–ª–æ—Ç–æ–≤) –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 7 –¥–Ω–µ–π –¥–æ transdate': 'logins_7d',
        '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ª–æ–≥–∏–Ω-—Å–µ—Å—Å–∏–π –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –¥–Ω–µ–π –¥–æ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏': 'logins_30d',
        '–°—Ä–µ–¥–Ω–µ–µ —á–∏—Å–ª–æ –ª–æ–≥–∏–Ω–æ–≤ –≤ –¥–µ–Ω—å –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 7 –¥–Ω–µ–π: logins_last_7_days / 7': 'avg_logins_7d',
        '–°—Ä–µ–¥–Ω–µ–µ —á–∏—Å–ª–æ –ª–æ–≥–∏–Ω–æ–≤ –≤ –¥–µ–Ω—å –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –¥–Ω–µ–π: logins_last_30_days / 30': 'avg_logins_30d',
        '–û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ —á–∞—Å—Ç–æ—Ç—ã –ª–æ–≥–∏–Ω–æ–≤ –∑–∞ 7 –¥–Ω–µ–π –∫ —Å—Ä–µ–¥–Ω–µ–π —á–∞—Å—Ç–æ—Ç–µ –∑–∞ 30 –¥–Ω–µ–π:\n(freq7d?freq30d)/freq30d(freq_{7d} - freq_{30d}) / freq_{30d}(freq7d?freq30d)/freq30d ‚Äî –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —Å—Ç–∞–ª –∫–ª–∏–µ–Ω—Ç –∑–∞—Ö–æ–¥–∏—Ç—å —á–∞—â–µ –∏–ª–∏ —Ä–µ–∂–µ –Ω–µ–¥–∞–≤–Ω–æ': 'rel_freq_change_7_30d',
        '–î–æ–ª—è –ª–æ–≥–∏–Ω–æ–≤ –∑–∞ 7 –¥–Ω–µ–π –æ—Ç –ª–æ–≥–∏–Ω–æ–≤ –∑–∞ 30 –¥–Ω–µ–π': 'login_share_7_30d',
        '–°—Ä–µ–¥–Ω–∏–π –∏–Ω—Ç–µ—Ä–≤–∞–ª (–≤ —Å–µ–∫—É–Ω–¥–∞—Ö) –º–µ–∂–¥—É —Å–æ—Å–µ–¥–Ω–∏–º–∏ —Å–µ—Å—Å–∏—è–º–∏ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –¥–Ω–µ–π': 'avg_login_interval',
        '–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤ –º–µ–∂–¥—É –ª–æ–≥–∏–Ω–∞–º–∏ –∑–∞ 30 –¥–Ω–µ–π (–≤ —Å–µ–∫—É–Ω–¥–∞—Ö), –∏–∑–º–µ—Ä—è–µ—Ç —Ä–∞–∑–±—Ä–æ—Å –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤': 'std_login_interval',
        '–ü–æ–∫–∞–∑–∞—Ç–µ–ª—å "–≤–∑—Ä—ã–≤–Ω–æ—Å—Ç–∏" –ª–æ–≥–∏–Ω–æ–≤: (std?mean)/(std+mean)(std - mean)/(std + mean)(std?mean)/(std+mean) –¥–ª—è –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤': 'login_volatility_factor',
        'Fano-factor –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤: variance / mean': 'fano_factor_interval',
        'Z-—Å–∫–æ—Ä —Å—Ä–µ–¥–Ω–µ–≥–æ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 7 –¥–Ω–µ–π –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Å—Ä–µ–¥–Ω–µ–≥–æ –∑–∞ 30 –¥–Ω–µ–π: –Ω–∞—Å–∫–æ–ª—å–∫–æ —Å–∏–ª—å–Ω–æ –Ω–µ–¥–∞–≤–Ω–∏–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã –æ—Ç–ª–∏—á–∞—é—Ç—Å—è –æ—Ç —Ç–∏–ø–∏—á–Ω—ã—Ö, –≤ –µ–¥–∏–Ω–∏—Ü–∞—Ö —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è': 'z_score_avg_interval_7d_vs_30d',
        '–≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –≤–∑–≤–µ—à–µ–Ω–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤ –º–µ–∂–¥—É –ª–æ–≥–∏–Ω–∞–º–∏ –∑–∞ 7 –¥–Ω–µ–π, –≥–¥–µ –±–æ–ª–µ–µ —Å–≤–µ–∂–∏–µ —Å–µ—Å—Å–∏–∏ –∏–º–µ—é—Ç –±–æ–ª—å—à–∏–π –≤–µ—Å (–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –∑–∞—Ç—É—Ö–∞–Ω–∏—è 0.3)': 'weighted_avg_interval_7d',
        '–î–∏—Å–ø–µ—Ä—Å–∏—è –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤ –º–µ–∂–¥—É –ª–æ–≥–∏–Ω–∞–º–∏ –∑–∞ 30 –¥–Ω–µ–π (–≤ —Å–µ–∫—É–Ω–¥–∞—Ö?), –µ—â—ë –æ–¥–Ω–∞ –º–µ—Ä–∞ —Ä–∞–∑–±—Ä–æ—Å–∞': 'interval_variance_30d',
    }
    df_behavior.rename(columns=behav_map, inplace=True)
    
    # Define expected numeric columns
    numeric_cols = [
        'amount',
        'os_count_30d', 'device_count_30d',
        'logins_7d', 'logins_30d',
        'avg_logins_7d', 'avg_logins_30d',
        'avg_login_interval', 'std_login_interval',
        'rel_freq_change_7_30d',
        'login_share_7_30d',
        'weighted_avg_interval_7d',
        'login_volatility_factor',
        'fano_factor_interval',
        'z_score_avg_interval_7d_vs_30d',
        'interval_variance_30d',
    ]
    
    # Force numeric types on known columns
    for df_temp in [df_trans, df_behavior]:
        for col in numeric_cols:
            if col in df_temp.columns:
                df_temp[col] = pd.to_numeric(df_temp[col], errors='coerce').fillna(0)
    
    # Normalize ID columns for merge
    for df_temp in [df_trans, df_behavior]:
        if 'cst_dim_id' in df_temp.columns:
            df_temp['cst_dim_id'] = (
                pd.to_numeric(df_temp['cst_dim_id'], errors='coerce')
                .fillna(0)
                .astype(int)
                .astype(str)
            )
        if 'transdate' in df_temp.columns:
            df_temp['transdate'] = pd.to_datetime(
                df_temp['transdate'].astype(str).str.strip("'"), errors='coerce'
            )
    
    # Merge datasets
    print("üîó Merging datasets...")
    df = df_trans.merge(df_behavior, on=['cst_dim_id', 'transdate'], how='left')
    
    # Fill categorical NaNs
    for c in ['last_phone_model', 'last_os_ver', 'direction']:
        if c in df.columns:
            df[c] = df[c].fillna('Unknown')
    
    # Final numeric cleaning
    for col in numeric_cols:
        if col in df.columns:
            df[col] = (
                pd.to_numeric(df[col].astype(str).str.replace(',', '.', regex=False), errors='coerce')
                .fillna(0)
                .astype(float)
            )
    
    df.fillna(0, inplace=True)
    print(f"‚úì Dataset ready: {df.shape}, Fraud rate: {df['target'].mean()*100:.2f}%")
    
    return df


def add_derived_features(df):
    """
    Add derived features that improve model performance.
    These features were identified as missing in the ML pipeline.
    
    Args:
        df: DataFrame with merged transaction and behavioral data
    
    Returns:
        pd.DataFrame: DataFrame with additional features
    """
    print("üîß Adding derived features...")
    df = df.copy()
    
    # =========================================================================
    # 1. device_count_30d - —É–∂–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –∏–∑ behavioral, –Ω–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º
    # =========================================================================
    if 'device_count_30d' not in df.columns:
        # –ï—Å–ª–∏ –Ω–µ—Ç - —Å—á–∏—Ç–∞–µ–º –∫–∞–∫ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤ –∑–∞ 30 –¥–Ω–µ–π
        if 'last_phone_model' in df.columns and 'cst_dim_id' in df.columns:
            device_counts = df.groupby('cst_dim_id')['last_phone_model'].transform('nunique')
            df['device_count_30d'] = device_counts
        else:
            df['device_count_30d'] = 1
        print("   ‚úì Added device_count_30d")
    
    # =========================================================================
    # 2. login_volatility_factor - (std - mean) / (std + mean) –¥–ª—è –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤
    # =========================================================================
    if 'login_volatility_factor' not in df.columns:
        if 'std_login_interval' in df.columns and 'avg_login_interval' in df.columns:
            std = pd.to_numeric(df['std_login_interval'], errors='coerce').fillna(0)
            mean = pd.to_numeric(df['avg_login_interval'], errors='coerce').fillna(0)
            # Avoid division by zero
            denominator = std + mean
            df['login_volatility_factor'] = np.where(
                denominator > 0,
                (std - mean) / denominator,
                0
            )
        else:
            df['login_volatility_factor'] = 0
        print("   ‚úì Added login_volatility_factor")
    
    # =========================================================================
    # 3. is_device_hopper - —Ñ–ª–∞–≥ —á–∞—Å—Ç–æ–π —Å–º–µ–Ω—ã —É—Å—Ç—Ä–æ–π—Å—Ç–≤ (>1 –∑–∞ 30 –¥–Ω–µ–π)
    # =========================================================================
    if 'is_device_hopper' not in df.columns:
        device_count = pd.to_numeric(df.get('device_count_30d', 1), errors='coerce').fillna(1)
        df['is_device_hopper'] = (device_count > 1).astype(int)
        print("   ‚úì Added is_device_hopper")
    
    # =========================================================================
    # 4. BONUS: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª–µ–∑–Ω—ã–µ —Ñ–∏—á–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è recall
    # =========================================================================
    
    # 4a. is_new_device - —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è —Å –Ω–æ–≤–æ–≥–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ (—Ä–µ–¥–∫–∞—è –º–æ–¥–µ–ª—å)
    if 'last_phone_model' in df.columns:
        device_freq = df['last_phone_model'].value_counts(normalize=True)
        df['is_rare_device'] = df['last_phone_model'].map(
            lambda x: 1 if device_freq.get(x, 0) < 0.01 else 0
        )
        print("   ‚úì Added is_rare_device")
    
    # 4b. login_burst - —Ä–µ–∑–∫–∏–π –≤—Å–ø–ª–µ—Å–∫ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ (7d >> 30d average)
    if 'logins_7d' in df.columns and 'logins_30d' in df.columns:
        logins_7d = pd.to_numeric(df['logins_7d'], errors='coerce').fillna(0)
        logins_30d = pd.to_numeric(df['logins_30d'], errors='coerce').fillna(0)
        avg_7d_expected = logins_30d / 4.28  # 30/7 = 4.28
        df['login_burst'] = np.where(
            avg_7d_expected > 0,
            logins_7d / avg_7d_expected,
            0
        )
        df['is_login_burst'] = (df['login_burst'] > 2.0).astype(int)  # 2x –Ω–æ—Ä–º—ã
        print("   ‚úì Added login_burst, is_login_burst")
    
    # 4c. is_high_amount - —Å—É–º–º–∞ –≤—ã—à–µ 90-–≥–æ –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—è
    if 'amount' in df.columns:
        amount = pd.to_numeric(df['amount'], errors='coerce').fillna(0)
        p90 = amount.quantile(0.90)
        df['is_high_amount'] = (amount > p90).astype(int)
        print(f"   ‚úì Added is_high_amount (threshold: {p90:,.0f})")
    
    # 4d. suspicious_time - —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è –≤ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –≤—Ä–µ–º—è (–Ω–æ—á—å 0-6)
    if 'transdatetime' in df.columns:
        df['transdatetime'] = pd.to_datetime(df['transdatetime'].astype(str).str.strip("'"), errors='coerce')
        df['hour'] = df['transdatetime'].dt.hour.fillna(12).astype(int)
        df['is_night_transaction'] = ((df['hour'] >= 0) & (df['hour'] <= 6)).astype(int)
        print("   ‚úì Added hour, is_night_transaction")
    
    # 4e. device_os_mismatch - –Ω–µ—Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ OS –∏ device (–ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ)
    if 'last_os_ver' in df.columns and 'last_phone_model' in df.columns:
        # iOS –Ω–∞ Android —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ –∏–ª–∏ –Ω–∞–æ–±–æ—Ä–æ—Ç
        is_ios = df['last_os_ver'].astype(str).str.lower().str.contains('ios|iphone', na=False)
        is_android_device = df['last_phone_model'].astype(str).str.lower().str.contains('samsung|xiaomi|huawei|oppo|vivo|realme|poco', na=False)
        df['device_os_mismatch'] = (is_ios & is_android_device).astype(int)
        print("   ‚úì Added device_os_mismatch")
    
    print(f"‚úì Derived features added. New shape: {df.shape}")
    return df


def preprocess(df):
    """
    Apply full preprocessing pipeline including derived features.
    
    Args:
        df: DataFrame to preprocess
    
    Returns:
        pd.DataFrame: Preprocessed DataFrame with all features
    """
    # Add derived features
    df = add_derived_features(df)
    
    return df