"""
Data Loading and Preprocessing Module
Centralized data loading to avoid code duplication
"""

import pandas as pd
import numpy as np
import os
import warnings
warnings.filterwarnings('ignore')


def clean_columns(df):
    """Remove spaces and BOM markers from column names."""
    df.columns = (
        df.columns.astype(str)
        .str.strip()
        .str.replace('\ufeff', '')
        .str.replace('"', '')
    )
    return df


def load_data(base_path='docs'):
    """
    Load transaction and behavioral data from CSV files.
    
    Args:
        base_path: Directory containing CSV files (default: 'docs')
    
    Returns:
        tuple: (df_transactions, df_behavioral)
    
    Raises:
        FileNotFoundError: If CSV files are not found
    """
    print("üìä Loading data...")
    
    # Try different paths
    trans_paths = [
        os.path.join(base_path, '—Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ –≤ –ú–æ–±–∏–ª—å–Ω–æ–º –∏–Ω—Ç–µ—Ä–Ω–µ—Ç –ë–∞–Ω–∫–∏–Ω–≥–µ.csv'),
        '—Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ –≤ –ú–æ–±–∏–ª—å–Ω–æ–º –∏–Ω—Ç–µ—Ä–Ω–µ—Ç –ë–∞–Ω–∫–∏–Ω–≥–µ.csv'
    ]
    
    behav_paths = [
        os.path.join(base_path, '–ø–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∫–ª–∏–µ–Ω—Ç–æ–≤.csv'),
        '–ø–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∫–ª–∏–µ–Ω—Ç–æ–≤.csv'
    ]
    
    # Load transactions
    df_trans = None
    for path in trans_paths:
        if os.path.exists(path):
            try:
                df_trans = pd.read_csv(
                    path,
                    sep=';',
                    encoding='cp1251',
                    header=1,
                    engine='python',
                )
                df_trans = clean_columns(df_trans)
                print(f"‚úì Loaded transactions from {path} (shape: {df_trans.shape})")
                break
            except Exception as e:
                print(f"‚ö†Ô∏è Failed to load {path}: {e}")
                continue
    
    if df_trans is None:
        raise FileNotFoundError(f"Transaction CSV not found in paths: {trans_paths}")
    
    # Load behavioral patterns
    df_behavior = None
    for path in behav_paths:
        if os.path.exists(path):
            try:
                df_behavior = pd.read_csv(
                    path,
                    sep=';',
                    encoding='cp1251',
                    header=0,
                    engine='python',
                    on_bad_lines='skip',
                )
                df_behavior = clean_columns(df_behavior)
                print(f"‚úì Loaded behavioral data from {path} (shape: {df_behavior.shape})")
                break
            except Exception as e:
                print(f"‚ö†Ô∏è Failed to load {path}: {e}")
                continue
    
    if df_behavior is None:
        raise FileNotFoundError(f"Behavioral CSV not found in paths: {behav_paths}")
    
    return df_trans, df_behavior


def clean_and_merge(df_trans, df_behavior):
    """
    Clean column names, normalize data types, and merge datasets.
    
    Args:
        df_trans: Transaction DataFrame
        df_behavior: Behavioral patterns DataFrame
    
    Returns:
        pd.DataFrame: Merged and cleaned dataset
    """
    print("üîß Cleaning and merging data...")
    
    # Rename behavioral columns for consistency
    behav_map = {
        '–£–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∫–ª–∏–µ–Ω—Ç–∞': 'cst_dim_id',
        '–î–∞—Ç–∞ —Å–æ–≤–µ—Ä—à–µ–Ω–Ω–æ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏': 'transdate',
        '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–∞–∑–Ω—ã—Ö –≤–µ—Ä—Å–∏–π –û–° (os_ver) –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –¥–Ω–µ–π –¥–æ transdate ‚Äî —Å–∫–æ–ª—å–∫–æ —Ä–∞–∑–Ω—ã—Ö –û–°/–≤–µ—Ä—Å–∏–π –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª –∫–ª–∏–µ–Ω—Ç': 'os_count_30d',
        '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Ç–µ–ª–µ—Ñ–æ–Ω–∞ (phone_model) –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –¥–Ω–µ–π ‚Äî –Ω–∞—Å–∫–æ–ª—å–∫–æ —á–∞—Å—Ç–æ –∫–ª–∏–µ–Ω—Ç "–º–µ–Ω—è–ª —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ" –ø–æ –ª–æ–≥–∞–º': 'device_count_30d',
        '–ú–æ–¥–µ–ª—å —Ç–µ–ª–µ—Ñ–æ–Ω–∞ –∏–∑ —Å–∞–º–æ–π –ø–æ—Å–ª–µ–¥–Ω–µ–π —Å–µ—Å—Å–∏–∏ (–ø–æ –≤—Ä–µ–º–µ–Ω–∏) –ø–µ—Ä–µ–¥ transdate': 'last_phone_model',
        '–í–µ—Ä—Å–∏—è –û–° –∏–∑ —Å–∞–º–æ–π –ø–æ—Å–ª–µ–¥–Ω–µ–π —Å–µ—Å—Å–∏–∏ –ø–µ—Ä–µ–¥ transdate': 'last_os_ver',
        '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ª–æ–≥–∏–Ω-—Å–µ—Å—Å–∏–π (–º–∏–Ω—É—Ç–Ω—ã—Ö —Ç–∞–π–º-—Å–ª–æ—Ç–æ–≤) –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 7 –¥–Ω–µ–π –¥–æ transdate': 'logins_7d',
        '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ª–æ–≥–∏–Ω-—Å–µ—Å—Å–∏–π –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –¥–Ω–µ–π –¥–æ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏': 'logins_30d',
        '–°—Ä–µ–¥–Ω–µ–µ —á–∏—Å–ª–æ –ª–æ–≥–∏–Ω–æ–≤ –≤ –¥–µ–Ω—å –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 7 –¥–Ω–µ–π: logins_last_7_days / 7': 'avg_logins_7d',
        '–°—Ä–µ–¥–Ω–µ–µ —á–∏—Å–ª–æ –ª–æ–≥–∏–Ω–æ–≤ –≤ –¥–µ–Ω—å –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –¥–Ω–µ–π: logins_last_30_days / 30': 'avg_logins_30d',
        '–û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ —á–∞—Å—Ç–æ—Ç—ã –ª–æ–≥–∏–Ω–æ–≤ –∑–∞ 7 –¥–Ω–µ–π –∫ —Å—Ä–µ–¥–Ω–µ–π —á–∞—Å—Ç–æ—Ç–µ –∑–∞ 30 –¥–Ω–µ–π:\n(freq7d?freq30d)/freq30d(freq_{7d} - freq_{30d}) / freq_{30d}(freq7d?freq30d)/freq30d ‚Äî –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —Å—Ç–∞–ª –∫–ª–∏–µ–Ω—Ç –∑–∞—Ö–æ–¥–∏—Ç—å —á–∞—â–µ –∏–ª–∏ —Ä–µ–∂–µ –Ω–µ–¥–∞–≤–Ω–æ': 'rel_freq_change_7_30d',
        '–î–æ–ª—è –ª–æ–≥–∏–Ω–æ–≤ –∑–∞ 7 –¥–Ω–µ–π –æ—Ç –ª–æ–≥–∏–Ω–æ–≤ –∑–∞ 30 –¥–Ω–µ–π': 'login_share_7_30d',
        '–°—Ä–µ–¥–Ω–∏–π –∏–Ω—Ç–µ—Ä–≤–∞–ª (–≤ —Å–µ–∫—É–Ω–¥–∞—Ö) –º–µ–∂–¥—É —Å–æ—Å–µ–¥–Ω–∏–º–∏ —Å–µ—Å—Å–∏—è–º–∏ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –¥–Ω–µ–π': 'avg_login_interval',
        '–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤ –º–µ–∂–¥—É –ª–æ–≥–∏–Ω–∞–º–∏ –∑–∞ 30 –¥–Ω–µ–π (–≤ —Å–µ–∫—É–Ω–¥–∞—Ö), –∏–∑–º–µ—Ä—è–µ—Ç —Ä–∞–∑–±—Ä–æ—Å –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤': 'std_login_interval',
        '–ü–æ–∫–∞–∑–∞—Ç–µ–ª—å "–≤–∑—Ä—ã–≤–Ω–æ—Å—Ç–∏" –ª–æ–≥–∏–Ω–æ–≤: (std?mean)/(std+mean)(std - mean)/(std + mean)(std?mean)/(std+mean) –¥–ª—è –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤': 'login_volatility_factor',
        'Fano-factor –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤: variance / mean': 'fano_factor_interval',
        'Z-—Å–∫–æ—Ä —Å—Ä–µ–¥–Ω–µ–≥–æ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 7 –¥–Ω–µ–π –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Å—Ä–µ–¥–Ω–µ–≥–æ –∑–∞ 30 –¥–Ω–µ–π: –Ω–∞—Å–∫–æ–ª—å–∫–æ —Å–∏–ª—å–Ω–æ –Ω–µ–¥–∞–≤–Ω–∏–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã –æ—Ç–ª–∏—á–∞—é—Ç—Å—è –æ—Ç —Ç–∏–ø–∏—á–Ω—ã—Ö, –≤ –µ–¥–∏–Ω–∏—Ü–∞—Ö —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è': 'z_score_avg_interval_7d_vs_30d',
        '–≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –≤–∑–≤–µ—à–µ–Ω–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤ –º–µ–∂–¥—É –ª–æ–≥–∏–Ω–∞–º–∏ –∑–∞ 7 –¥–Ω–µ–π, –≥–¥–µ –±–æ–ª–µ–µ —Å–≤–µ–∂–∏–µ —Å–µ—Å—Å–∏–∏ –∏–º–µ—é—Ç –±–æ–ª—å—à–∏–π –≤–µ—Å (–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –∑–∞—Ç—É—Ö–∞–Ω–∏—è 0.3)': 'weighted_avg_interval_7d',
        '–î–∏—Å–ø–µ—Ä—Å–∏—è –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤ –º–µ–∂–¥—É –ª–æ–≥–∏–Ω–∞–º–∏ –∑–∞ 30 –¥–Ω–µ–π (–≤ —Å–µ–∫—É–Ω–¥–∞—Ö?), –µ—â—ë –æ–¥–Ω–∞ –º–µ—Ä–∞ —Ä–∞–∑–±—Ä–æ—Å–∞': 'interval_variance_30d',
    }
    df_behavior.rename(columns=behav_map, inplace=True)
    
    # Define expected numeric columns
    numeric_cols = [
        'amount',
        'os_count_30d', 'device_count_30d',
        'logins_7d', 'logins_30d',
        'avg_logins_7d', 'avg_logins_30d',
        'avg_login_interval', 'std_login_interval',
        'rel_freq_change_7_30d',
        'login_share_7_30d',
        'weighted_avg_interval_7d',
        'login_volatility_factor',
        'fano_factor_interval',
        'z_score_avg_interval_7d_vs_30d',
        'interval_variance_30d',
    ]
    
    # Force numeric types on known columns
    for df_temp in [df_trans, df_behavior]:
        for col in numeric_cols:
            if col in df_temp.columns:
                df_temp[col] = pd.to_numeric(df_temp[col], errors='coerce').fillna(0)
    
    # Normalize ID columns for merge
    for df_temp in [df_trans, df_behavior]:
        if 'cst_dim_id' in df_temp.columns:
            df_temp['cst_dim_id'] = (
                pd.to_numeric(df_temp['cst_dim_id'], errors='coerce')
                .fillna(0)
                .astype(int)
                .astype(str)
            )
        if 'transdate' in df_temp.columns:
            df_temp['transdate'] = pd.to_datetime(
                df_temp['transdate'].astype(str).str.strip("'"), errors='coerce'
            )
    
    # Merge datasets
    print("üîó Merging datasets...")
    df = df_trans.merge(df_behavior, on=['cst_dim_id', 'transdate'], how='left')
    
    # Fill categorical NaNs
    for c in ['last_phone_model', 'last_os_ver', 'direction']:
        if c in df.columns:
            df[c] = df[c].fillna('Unknown')
    
    # Final numeric cleaning
    for col in numeric_cols:
        if col in df.columns:
            df[col] = (
                pd.to_numeric(df[col].astype(str).str.replace(',', '.', regex=False), errors='coerce')
                .fillna(0)
                .astype(float)
            )
    
    df.fillna(0, inplace=True)
    print(f"‚úì Dataset ready: {df.shape}, Fraud rate: {df['target'].mean()*100:.2f}%")
    
    return df


def preprocess(df):
    """
    Apply additional preprocessing steps (placeholder for future enhancements).
    
    Args:
        df: DataFrame to preprocess
    
    Returns:
        pd.DataFrame: Preprocessed DataFrame
    """
    # Future: scaling, encoding, additional cleaning
    return df